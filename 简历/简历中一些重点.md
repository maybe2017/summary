
## 调优部分


### 通用
1. 通过实现 LRU 缓存部分热点商品数据，避免业务层缓存全量商品数据，减轻数据库压力及缓存占用。


### 质检流程
(1) 完成质检全流程监控、容错、与一致性方案升级:
1. 幂等拦截: 通过Snowflake实现请求唯一ID校验，流程重复执行率降低到 0%
2. 自动补偿: 异常事务自动重试(最大3次) + 人工兜底通道，实现数据的最终一致性。
3. 监控升级: 参数标准化，统一计费消息log id，计费链路追踪，支持日志回放功能，满足消息补扣，风控规则测试，计费策略测试等 ，报警支持消耗分钟、小时、日级报警。通过ELK、SkyWalking定位慢请求。

TODO 监控怎么做？
TODO 数据比对方案？



(2) 多级缓存优化: 采用本地缓存 (Cafeine) + 分布式缓存(Redis) + ES冷热分层策略，ES查询压力降低70%，实现99.9%请求响应<10ms。


### ES部分
1. 系统重构，引入 Canal 同步MongoDB数据至 Elasticsearch， Elasticsearch处理音频转写文本搜索、质检得分聚合等场景，读写分离，使相关部分的查询接口响应时间从秒级优化到毫秒级。

###
1. 独自完成 MongoDB 分片集群的部署及代码功能整合，解决了日均千万级的消息数据入库、后续分析流程阻塞困境。

##### 完成消息及会话等数据存储架构升级，完成会话消息海量数据的分库分表设计+异构存储架构，实现海量数据场景下的高性能读写，打造高性能的sql+ nosql，业务查询从过去的800ms缩减到40ms性能提升20倍。

(1)问题: 之前系统中用mysql存储用户与管家的对话消息数据，在2亿以上规模后，会话及消息表查询速度太慢了，在演进过程中，完成了两次大的升级，彻底解决了10亿-100亿以扣费流水海量数据存储维度的高性能问题。
(2)完成扣费中台分库分表升级，面向1万tps高并发，实现设计和实现分库分表方案，按机构id分库，按时间分表。口完成高性能分库分表SDK组件选型和定制化改造:调研分库分表中间件，基于能够灵活自由的角度，选用 ShardinglDBC 实现分库分表，采用 Shardingsphere-JDBC 的Hint 分片策略，自定义分库和分表的路由算法，用最大灵活度实现分片策略.完成组合模式分库分表方案设计:机构ID作为分库键、时间范用作为分表键。机构D取模分库，将数据均匀分布到多个数据库(8个库提供1Wto并发)中，按时间范用分表，只保留近3个月热数据，将数据按时间段划分到不同的表中，历史数据迁移至CIkhouse.
(3)完成扣费中台分库分表数据迁移，设计新旧双写+灰度切流+三级校验方案的，实现在线迁移的时候新旧双写，阶梯灰度，顺利实现丝滑扩容。
配合使用ookeeper的在线控制机制，实现配置的版本控制和回滚机制、灰度开关控制，防止因配置错误导致系统故障.结合xxl-job完成三级校验策略的设计与落地: 字段级校验(小时级)选取表中关键字段(如主键、时间戳、业务核心字段)作为比对基准，避免全字段比对带来的性能损耗。行级校验(每日低谷期)通过MD5函数生成哈希值，完成增量数据的md5比对。