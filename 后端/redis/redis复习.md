### 架构上的理解

#### PS MQ，为什么MQ的读节点不能提供读能力？
1. Redis是高性能的代表，是缓存/数据库，允许一定程度的数据不一致（如异步复制），因此从节点可以提供读能力（最终一致性）。【读多写少】
2. RabbitMQ 是消息队列，核心目标是 消息的可靠投递和严格顺序性，必须保证所有操作（包括读）经过主节点，需要强一致性。【写多读少】
3. 两者的读语义不一样
	```
	Redis 的"读"是幂等的
	RabbitMQ 的"读"是消费行为，会改变队列状态


	```


### 一、持久化

#### 进程的fork
1. 复制的什么? 进程描述符和页表
	```
	// 进程描述符：内核中数据结构task_struct
	struct task_struct {
		pid_t pid, tgid; 			// 进程id，组id
		volatile long state;        // 进程状态（如TASK_RUNNING、TASK_INTERRUPTIBLE
		struct mm_struct *mm;       // 指向内存管理结构（包含页表、虚拟内存区域等）
		struct files_struct *files; // 打开的文件描述符表
	};

	// 页表：将进程的虚拟内存地址映射到物理内存地址

	父进程
	├─ task_struct (PID=100)
	├─ 页表 (虚拟地址 → 物理页A/B/C)
	└─ 物理内存 [页A][页B][页C]

	fork() 后：
	
	父进程                         子进程
	├─ task_struct (PID=100)      ├─ task_struct (PID=101)  # 复制描述符
	├─ 页表 (read-only)            ├─ 页表 (read-only)        # 复制页表，权限都设为`read-only`，意味着 `父子进程都只能读`
	└─ 物理内存 [页A][页B][页C]      └─ 指向相同的物理页A/B/C  # 共享内存，不复制实际物理内存页

    
    父子任意进程进行写内存时，CPU硬件检测到内存页是read-only的，于是触发页异常中断`page-fault`，陷入kernel的一个中断例程。
    中断例程中，kernel就会[仅仅]把`触发的异常的页`复制一份，于是父子进程各自持有独立的一份。
	```

2. 优缺点
	```
	减少分配和复制大量资源时带来的瞬间延时；
	如果在fork()之后，父子进程都进行频繁写操作，那么会产生大量的分页错误（页异常中断page-fault），所以执行`bgsave`或`bgRewriteAof`命令时，会提高`rehash阶段负载因子`的阈值，避免哈希表进行扩容
	```

#### RDB(快照形式)【可能丢数据、耗大量cpu及内存资源】
1. 触发时机
	```
	// 手动触发 save bgsave 命令
	// 自动触发
	save 900 1      # 900秒内至少1次修改
	save 300 10     # 300秒内至少10次修改
	save 60 10000   # 60秒内至少10000次修改

	// 被动触发
	主从复制：从节点首次全量同步时，主节点自动触发 BGSAVE。
	SHUTDOWN 命令：若无 AOF 配置，默认执行 SAVE 保存数据。
	DEBUG RELOAD：重启时若开启 save 参数，触发 RDB 保存。
	```

2. RDB文件生成流程
	```
	1. fork出子进程后台执行 【短暂阻塞 - 10GB数据20ms左右】
	2. 子进程遍历内存所有数据，按特定格式写入临时文件，然后原子替换旧的文件。

	// 在第二步执行过程中，主线程依然可能发生写入，这部分新来的写入内容，子进程不可能看到了。【重点：父进程的增量数据子进程看不见】
	// 父进程修改数据时，触发COW，内核为`父进程`分配新的物理内存页，子进程仍保留对原页的引用。
	```
3. 优缺点
	```
	二进制压缩后体积小（适合定期全量归档备份）
	恢复速度快（比 AOF 快数倍）
	数据丢失风险：两次快照间的数据可能丢失（依赖配置阈值）
	```

#### AOF(命令日志文件)【主要消耗磁盘io，重写的时候耗cpu及内存资源】
#### 1. AOF缓冲区刷盘频率(主进程)
	appendfsync always ：写内存后立即写磁盘
	appendfsync everysec ：先写到AOF缓冲区，然后每隔一秒写到AOF文件【最坏丢1s】
	appendfsync no ： 先写到AOF缓冲区，由操作系统决定刷盘时机(30S)

#### 2. AOF文件重写
1. 重写时机：文件达到了某个大小或增长到固定比例后
2. 重写目的：精简命令，压缩AOF文件大小
	```
	// 合并单条命令，取最新设值
	// 合并多条命令 => 一条批量命令
	```
2. 重写过程：
	```
	1. fork出子进程后台执行 【短暂阻塞 - 10GB数据20ms左右】
	2. 在旧AOF文件基础上，结合redis中数据的实际情况，精简命令，生成新 AOF 文件。

	// 在第二步执行过程中，主线程依然可能发生写入，这部分新来的写入内容，会被主进程复制写入 `AOF重写缓冲区`【重点：父进程的增量数据子进程看不见，所以需要缓冲区】
	3. 主进程将重写缓冲区中的命令追加到新 AOF 文件中，然后原子替换旧的文件。
	```

### 二、内存淘汰

#### 8种内存淘汰策略
1. 前提参数 `maxmemory: 0`，默认值为0，内存满时，拒绝写入，但是可读可删。
	```
	// 当修改了该参数值后，表示达到阈值后，会用配置的策略进行内存淘汰。
	maxmemory 2GB                   # 内存上限
	maxmemory-policy volatile-lru   # 淘汰策略（默认noeviction）
	maxmemory-samples 5             # 采样精度（LRU/LFU等策略使用）

	// 第一大类：仅对 `设置了过期时间TTL` 的数据进行淘汰
	allkeys-lru			淘汰最近最少使用（Least Recently Used）的键。
	allkeys-lfu			淘汰最不频繁使用（Least Frequently Used）的键
	allkeys-random		随机

	// 第二大类：对 `全体数据` 进行淘汰
	volatile-lru		淘汰最近最少使用的键
	volatile-lfu		淘汰最不频繁使用的键
	volatile-random		随机
	volatile-ttl		淘汰剩余存活时间最短的键（TTL最小的优先）

	// 对于LRU策略，Redis 没有实现精确LRU 【短期访问模式变化快的数据 - 操作系统内存页面】
	精确LRU需要维护链表，单线程环境下性能差
	Redis会 随机采样 maxmemory-samples 个键（默认5个），然后淘汰采样结果中最久未访问的键。（samples=10时已近似LRU）
	
	// LFU 最不频繁使用 【避免短期突增干扰，更精准识别长期热点数据】
	统计访问频率：每个key维护一个计数器，且计数器随时间衰减（访问key时不会重置计数器，而是概率性增加计数器值）
	淘汰时，优先淘汰计数器值最小的键
	```


#### 过期key删除策略
1. 过期key会被转移到一个hash表中，定时扫描删除(贪心策略)，随机选20个key删除，如果过期的key仍然超过1/4，会继续随机删除。
2. 惰性删除，即访问时删除。


#### 内存碎片化问题
1. 底层通过c语音的`jeMalloc函数`分配的内存，固定大小字节(8,16,32,64)，有时一个对象不能完全使用完申请的内存，所以会产生内存碎片。
2. 启动内存碎片整理：`activedefrag: no` 【默认值是不启动，因为会耗性能】


### 三、主从集群【读写分离，提高并发读能力】

#### 一些配置参数或术语含义
1. replicationID：每一个节点都有一个唯一的`replicationID`，用以标识数据集版本
2. Replication Buffer：复制缓冲区
3. repl_baklog：复制积压缓冲区，主节点维护一个固定大小的环形缓冲区，保存最近传播的写命令，每次传播写命令时，主节点的offset偏移量增加。
4. offset：复制偏移量，标识同步`repl_baklog`的进度，主从节点均维护了一个值，记录已同步的数据量（字节数）
5. partial sync：增量同步

#### 数据同步【首次全量同步，后面增量同步】
1. slave先发送`partial sync`请求尝试进行增量同步，如果master判断replicationID不一致，会fork进行bgsave生成rdb文件，发送给slave进行`全量同步`，slave会清空本地数据，加载rdb文件。
2. rdb文件网络传输期间的新写命令会存入 复制缓冲区（Replication Buffer）
3. 全量同步完成后，主节点持续发送缓冲区`repl_baklog`保存的命令，异步发送给所有从节点（通过TCP长连接）
	```
	// 从节点断线重连后，发送自己的偏移量给主节点：
	1. 若偏移量仍在 backlog 中，主节点发送缺失的命令（增量同步）。
	2. 若偏移量已溢出，触发全量同步

	// repl-backlog-size：复制积压缓冲区大小（默认1MB）
	1. repl_baklog理解为一个环形。master的进度offset >= slave的offset，两个offset的差距就是slave待进行同步的命令集。
   	2. 如果slave断开太久，导致master中的`repl_baklog`文件写满后已经覆盖了slave未进行同步的数据，那么slave重启连上后，只能重新进行全量数据同步。
	```

#### 数据同步的问题及优化【尽量避免进行全量同步】
1. 数据不一致，频繁触发全量同步
	```
	1. 监控 master_repl_offset 和 slave_repl_offset 的差值。
	2. 增大 repl-backlog-size（如 256MB）
	3. 配置`repl-diskless-sync yes`启用：不进行磁盘复制(生成rdb文件)，直接发送网络io请求【前提网络要好】
	```
2. 从节点数较多，主节点写入压力大
	```
	限制一个master上slave的数量，可以采用 `master -> slave -> slave`的链式集群配置，减少master压力
	```

#### 主从架构下的锁丢失问题
1. 主从架构中，分布式锁key复制延迟问题？master写入锁后，还未进行同步就宕机，slave没有锁信息，成为新主后，会被写入重复的锁key
解决:【redisson的联锁multiLock，要从每一个节点中都获取到锁，才返回获取锁成功】
2. redLock是什么？

### 四、哨兵模式集群【主从集群 - 从仅做HA备份用，不适合读写分离】

#### 哨兵的作用【客户端的读写请求直接连接主节点（Master）或从节点（Slave），而不会经过哨兵节点】
1. 提供主节点地址查询服务。【服务发现】
2. 监控节点健康并执行故障转移。【监控及故障转移】

#### 监控（3个定时任务）
1. INFO命令：向所有主从节点发送 INFO 命令获取拓扑信息。（哨兵启动不用配置从节点信息，因为可以通过主节点获取）【每10s一次】
2. 发布订阅：主节点拥有一个频道 `_sentinel:hello_`，各个sentinel之间可通过发布自身信息及订阅来互相通信。【每2s一次】
3. PING心跳：sentinel会与所有节点维持心跳检活。【每1s一次】
	```
	// 响应超时判定主观下线
	sentinel down-after-milliseconds mymaster 5000  # 可以为不同角色设置不同的超时阈值

	// 客观下线
	当某个哨兵判断主观下线后，就会给其他哨兵发送 ` is-master-down-by-addr` 命令。接着其他哨兵会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N
	相当于反对票，当超过 quorum 都认为主观下线后，即认为客观下线
	```
#### 自动故障转移
1. 选举领导者 Sentinel：使用 Raft 算法选出一个 Sentinel 负责故障转移。
2. 选新主节点：领导者Sentinel会从剩下的 slaver中选主。【断开时间短 >> 优先级配置高 >> 复制偏移offset值大 >> 节点运行ID小】
	```
	1. sentinel给备选的slave发送 `slave on one`，让其成为 master。【>5.0版本后是replicaof no one】
   	2. sentinel通过频道广播给其他slave， 发送 `slaveof newMasterIp`，让其他slave成为新master的从节点，开始从新master同步数据
   	3. sentinel将故障节点标记为slave(会修改配置文件，添加一行slaveof newMasterIp)，当故障节点重连后，自动成为slave节点
	```

#### 通知
1. 集群发生故障转移(选主)后，会通知给Java客户端【sentinel相当于服务发现注册中心】
2. 客户端会缓存主节点信息，但是客户端也会监听 Sentinel 的 +switch-master 事件实时感知变更，自动更新连接池指向新主节点，不必手动更改主节点配置。
	```
	spring:
	  redis:
		sentinel:
		master: mymaster  # Sentinel中监控的主节点逻辑名称（由sentinel monitor配置决定）
		nodes: 192.168.1.101:26379,192.168.1.102:26379,192.168.1.103:26379  # Sentinel节点列表
	```
#### 缺点：不适合做读写分离【不够高效】
1. 读请求虽然转移到了从节点，但是从节点的数据是全量的（不如分片集群中的从节点读效率高）
2. 哨兵集群的客户端，通常只负责故障转移，不自动支持读写分离，需要业务代码自己实现。

### 五、分片集群【解决单Master内存上限 + 分片更适合读写分离】
1. 数据分散存储到`多个master`上，实现了海量存储、提高并发写能力，也可为每个master加slave提高整个系统的并发读能力【读写分离】
2. 不需要sentinel了，多个master之间会互相发送心跳，且客户端的请求会被路由到正确的节点

#### 散列插槽(Gossip 协议通信)
1. 为什么仅设置16384 个槽?
	```
	// 足够细粒度分布数据，同时保持元数据轻量（仅需 2KB 位图传输）
	1. 16384 个槽已足够将数据均匀分布到多个节点
	2. Gossip 协议开销：节点间通过 Gossip 协议同步槽分配信息时，需广播 16384 / 8 = 2048 字节（2KB）的位图（bitmap）【够大了】
	3. 槽数越多，故障转移时重新分配槽的耗时越长
	
	// 2^14 = 16384，2^16 = 65536
	CRC16 生成的哈希值为 16 位（0~65535）
	slot = CRC16(key) % 16384

	// 槽的作用是数据分片，而非直接存储数据
	槽与节点的绑定：集群动态分配槽给节点（如节点A管理槽0~5000），但槽本身不存储数据，实际数据仍由节点存储
	```
2. 为什么存储的数据与 插槽slot绑定，而不是与节点绑定？
	```
	若某节点负载过高、或宕机(集群伸缩)，可仅迁移其管理的部分槽到其他节点，无需全量迁移
	扩容时：需手动转移部分槽到新节点
	redis-cli --cluster reshard 192.168.1.100:6379 --cluster-from all --cluster-to <new-node-id> --cluster-slots 1000
	```
3. 如何将同一类数据存储在同一个master节点呢？key怎么设计?
   	```
	// 核心原理：哈希标签（Hash Tag）
	Redis Cluster 允许通过 {} 指定仅对键的一部分计算哈希值，从而强制某些键落到同一个槽。
	如：
	user:{100}:profile		所有用户ID为100的数据归到同一槽
	order:{2023}:details	所有2023年订单归到同一槽
	```

#### 缺点
1. 不支持多键操作：跨槽的 MGET、MSET 等命令需拆分
2. 不支持跨节点事务
3. 扩容需要手动迁移槽